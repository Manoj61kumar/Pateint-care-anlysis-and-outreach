{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 1,000,000 records\n",
      "Training Size: 800,000, Testing Size: 200,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Narayanan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.3396 - loss: 1.8425 - val_accuracy: 0.4475 - val_loss: 0.9689\n",
      "Epoch 2/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.4465 - loss: 0.9781 - val_accuracy: 0.4495 - val_loss: 0.9576\n",
      "Epoch 3/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4485 - loss: 0.9660 - val_accuracy: 0.4481 - val_loss: 0.9581\n",
      "Epoch 4/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4490 - loss: 0.9625 - val_accuracy: 0.4483 - val_loss: 0.9574\n",
      "Epoch 5/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4489 - loss: 0.9613 - val_accuracy: 0.4487 - val_loss: 0.9559\n",
      "Epoch 6/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4492 - loss: 0.9594 - val_accuracy: 0.4490 - val_loss: 0.9561\n",
      "Epoch 7/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4491 - loss: 0.9588 - val_accuracy: 0.4496 - val_loss: 0.9569\n",
      "Epoch 8/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4494 - loss: 0.9571 - val_accuracy: 0.4485 - val_loss: 0.9573\n",
      "Epoch 9/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4499 - loss: 0.9580 - val_accuracy: 0.4473 - val_loss: 0.9551\n",
      "Epoch 10/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4505 - loss: 0.9569 - val_accuracy: 0.4494 - val_loss: 0.9555\n",
      "Epoch 11/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4495 - loss: 0.9578 - val_accuracy: 0.4486 - val_loss: 0.9554\n",
      "Epoch 12/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4500 - loss: 0.9559 - val_accuracy: 0.4500 - val_loss: 0.9546\n",
      "Epoch 13/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4510 - loss: 0.9563 - val_accuracy: 0.4471 - val_loss: 0.9552\n",
      "Epoch 14/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4498 - loss: 0.9558 - val_accuracy: 0.4488 - val_loss: 0.9575\n",
      "Epoch 15/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4497 - loss: 0.9569 - val_accuracy: 0.4485 - val_loss: 0.9545\n",
      "Epoch 16/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4510 - loss: 0.9547 - val_accuracy: 0.4468 - val_loss: 0.9544\n",
      "Epoch 17/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4515 - loss: 0.9555 - val_accuracy: 0.4489 - val_loss: 0.9544\n",
      "Epoch 18/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4511 - loss: 0.9551 - val_accuracy: 0.4477 - val_loss: 0.9558\n",
      "Epoch 19/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4523 - loss: 0.9548 - val_accuracy: 0.4491 - val_loss: 0.9545\n",
      "Epoch 20/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4517 - loss: 0.9548 - val_accuracy: 0.4482 - val_loss: 0.9546\n",
      "Epoch 21/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4524 - loss: 0.9544 - val_accuracy: 0.4461 - val_loss: 0.9570\n",
      "Epoch 22/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4513 - loss: 0.9546 - val_accuracy: 0.4484 - val_loss: 0.9537\n",
      "Epoch 23/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4536 - loss: 0.9536 - val_accuracy: 0.4488 - val_loss: 0.9543\n",
      "Epoch 24/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4540 - loss: 0.9534 - val_accuracy: 0.4491 - val_loss: 0.9532\n",
      "Epoch 25/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4527 - loss: 0.9537 - val_accuracy: 0.4488 - val_loss: 0.9549\n",
      "Epoch 26/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4543 - loss: 0.9529 - val_accuracy: 0.4497 - val_loss: 0.9541\n",
      "Epoch 27/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 24ms/step - accuracy: 0.4532 - loss: 0.9534 - val_accuracy: 0.4489 - val_loss: 0.9545\n",
      "Epoch 28/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.4531 - loss: 0.9535 - val_accuracy: 0.4485 - val_loss: 0.9534\n",
      "Epoch 29/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.4544 - loss: 0.9531 - val_accuracy: 0.4492 - val_loss: 0.9545\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4484 - loss: 0.9528\n",
      "Test Accuracy: 0.4491"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model, Encoders, Scaler, and Multi-Hot Encoder saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "\n",
    "# --------------------------------\n",
    "# 1. Load Dataset\n",
    "# --------------------------------\n",
    "df = pd.read_csv('million_patients.csv')\n",
    "print(f\"Loaded dataset with {len(df):,} records\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2. Preprocessing\n",
    "# --------------------------------\n",
    "# Drop unwanted columns\n",
    "df.drop(['patient_id', 'admit_date', 'discharge_date', 'los'], axis=1, inplace=True)\n",
    "\n",
    "# Encode Categorical Data\n",
    "label_encoders = {}\n",
    "categorical_cols = ['gender', 'region', 'diagnosis', 'category', 'medication', 'outcome']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Convert Symptoms to Multi-Hot Encoding\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: x.split(', '))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "symptom_encoded = pd.DataFrame(mlb.fit_transform(df['symptoms']), columns=mlb.classes_)\n",
    "\n",
    "# Merge the symptoms into the main DataFrame\n",
    "df = pd.concat([df.drop('symptoms', axis=1), symptom_encoded], axis=1)\n",
    "\n",
    "# Scale Numeric Columns\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = ['age', 'bmi', 'glucose', 'systolic_bp', 'diastolic_bp', 'wbc']\n",
    "\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Split Features and Labels\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# Split into Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training Size: {len(X_train):,}, Testing Size: {len(X_test):,}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 3. Build the Model\n",
    "# --------------------------------\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y)), activation='softmax')  # Output layer with # of diseases\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# --------------------------------\n",
    "# 4. Train the Model\n",
    "# --------------------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=2048,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# 5. Evaluate the Model\n",
    "# --------------------------------\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 6. Save the Model and Encoders\n",
    "# --------------------------------\n",
    "model.save('disease_prediction_model.h5')\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(mlb, 'multi_label_binarizer.pkl')\n",
    "\n",
    "print(\"✅ Model, Encoders, Scaler, and Multi-Hot Encoder saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 1,000,000 records\n",
      "Training Size: 800,000, Testing Size: 200,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Narayanan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.3376 - loss: 1.8664 - val_accuracy: 0.4490 - val_loss: 0.9687\n",
      "Epoch 2/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4478 - loss: 0.9773 - val_accuracy: 0.4475 - val_loss: 0.9591\n",
      "Epoch 3/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4487 - loss: 0.9665 - val_accuracy: 0.4479 - val_loss: 0.9579\n",
      "Epoch 4/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.4489 - loss: 0.9629 - val_accuracy: 0.4476 - val_loss: 0.9557\n",
      "Epoch 5/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4497 - loss: 0.9604 - val_accuracy: 0.4487 - val_loss: 0.9567\n",
      "Epoch 6/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.4490 - loss: 0.9598 - val_accuracy: 0.4477 - val_loss: 0.9553\n",
      "Epoch 7/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.4494 - loss: 0.9595 - val_accuracy: 0.4494 - val_loss: 0.9562\n",
      "Epoch 8/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.4494 - loss: 0.9585 - val_accuracy: 0.4470 - val_loss: 0.9563\n",
      "Epoch 9/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.4491 - loss: 0.9589 - val_accuracy: 0.4474 - val_loss: 0.9559\n",
      "Epoch 10/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.4516 - loss: 0.9571 - val_accuracy: 0.4488 - val_loss: 0.9562\n",
      "Epoch 11/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.4495 - loss: 0.9569 - val_accuracy: 0.4496 - val_loss: 0.9547\n",
      "Epoch 12/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.4484 - loss: 0.9570 - val_accuracy: 0.4482 - val_loss: 0.9548\n",
      "Epoch 13/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.4497 - loss: 0.9567 - val_accuracy: 0.4485 - val_loss: 0.9547\n",
      "Epoch 14/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.4503 - loss: 0.9557 - val_accuracy: 0.4491 - val_loss: 0.9551\n",
      "Epoch 15/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.4509 - loss: 0.9547 - val_accuracy: 0.4486 - val_loss: 0.9548\n",
      "Epoch 16/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.4503 - loss: 0.9558 - val_accuracy: 0.4483 - val_loss: 0.9544\n",
      "Epoch 17/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.4517 - loss: 0.9544 - val_accuracy: 0.4490 - val_loss: 0.9545\n",
      "Epoch 18/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.4516 - loss: 0.9554 - val_accuracy: 0.4500 - val_loss: 0.9546\n",
      "Epoch 19/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.4523 - loss: 0.9550 - val_accuracy: 0.4476 - val_loss: 0.9544\n",
      "Epoch 20/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.4520 - loss: 0.9545 - val_accuracy: 0.4484 - val_loss: 0.9543\n",
      "Epoch 21/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.4521 - loss: 0.9546 - val_accuracy: 0.4473 - val_loss: 0.9553\n",
      "Epoch 22/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.4522 - loss: 0.9543 - val_accuracy: 0.4475 - val_loss: 0.9544\n",
      "Epoch 23/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.4514 - loss: 0.9543 - val_accuracy: 0.4475 - val_loss: 0.9539\n",
      "Epoch 24/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.4516 - loss: 0.9534 - val_accuracy: 0.4479 - val_loss: 0.9547\n",
      "Epoch 25/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.4531 - loss: 0.9535 - val_accuracy: 0.4499 - val_loss: 0.9541\n",
      "Epoch 26/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.4523 - loss: 0.9542 - val_accuracy: 0.4475 - val_loss: 0.9547\n",
      "Epoch 27/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.4531 - loss: 0.9536 - val_accuracy: 0.4485 - val_loss: 0.9545\n",
      "Epoch 28/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.4528 - loss: 0.9536 - val_accuracy: 0.4487 - val_loss: 0.9544\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4461 - loss: 0.9537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4475\n",
      "✅ Model, Encoders, Scaler, and Multi-Hot Encoder saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "\n",
    "# --------------------------------\n",
    "# 1. Load Dataset\n",
    "# --------------------------------\n",
    "df = pd.read_csv('million_patients.csv')\n",
    "print(f\"Loaded dataset with {len(df):,} records\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2. Preprocessing\n",
    "# --------------------------------\n",
    "# Drop unwanted columns\n",
    "df.drop(['patient_id', 'admit_date', 'discharge_date', 'los'], axis=1, inplace=True)\n",
    "\n",
    "# Encode Categorical Data\n",
    "label_encoders = {}\n",
    "categorical_cols = ['gender', 'region', 'diagnosis', 'category', 'medication', 'outcome']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Convert Symptoms to Multi-Hot Encoding\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: x.split(', '))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "symptom_encoded = pd.DataFrame(mlb.fit_transform(df['symptoms']), columns=mlb.classes_)\n",
    "\n",
    "# Merge the symptoms into the main DataFrame\n",
    "df = pd.concat([df.drop('symptoms', axis=1), symptom_encoded], axis=1)\n",
    "\n",
    "# Scale Numeric Columns\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = ['age', 'bmi', 'glucose', 'systolic_bp', 'diastolic_bp', 'wbc']\n",
    "\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# Split Features and Labels\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# Split into Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training Size: {len(X_train):,}, Testing Size: {len(X_test):,}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 3. Build the Model\n",
    "# --------------------------------\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y)), activation='softmax')  # Output layer with # of diseases\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# --------------------------------\n",
    "# 4. Train the Model\n",
    "# --------------------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=2048,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# 5. Evaluate the Model\n",
    "# --------------------------------\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 6. Save the Model and Encoders\n",
    "# --------------------------------\n",
    "model.save('model/disease_prediction_model.h5')\n",
    "joblib.dump(label_encoders, 'model/label_encoders.pkl')\n",
    "joblib.dump(scaler, 'model/scaler.pkl')\n",
    "joblib.dump(mlb, 'model/multi_label_binarizer.pkl')\n",
    "\n",
    "print(\"✅ Model, Encoders, Scaler, and Multi-Hot Encoder saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 1,000,000 records\n",
      "Columns in dataset: ['patient_id', 'age', 'gender', 'region', 'bmi', 'diagnosis', 'category', 'severity', 'symptoms', 'glucose', 'systolic_bp', 'diastolic_bp', 'wbc', 'admit_date', 'los', 'discharge_date', 'medication', 'outcome']\n",
      "Numeric features scaled.\n",
      "Categorical features encoded.\n",
      "Symptoms encoded successfully.\n",
      "Training size: (800000, 130), Testing size: (200000, 130)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Narayanan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 25ms/step - accuracy: 0.3448 - loss: 1.8132 - val_accuracy: 0.4478 - val_loss: 0.9759\n",
      "Epoch 2/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.4467 - loss: 0.9768 - val_accuracy: 0.4493 - val_loss: 0.9596\n",
      "Epoch 3/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.4488 - loss: 0.9655 - val_accuracy: 0.4491 - val_loss: 0.9565\n",
      "Epoch 4/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.4480 - loss: 0.9637 - val_accuracy: 0.4486 - val_loss: 0.9561\n",
      "Epoch 5/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.4493 - loss: 0.9607 - val_accuracy: 0.4482 - val_loss: 0.9567\n",
      "Epoch 6/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.4494 - loss: 0.9597 - val_accuracy: 0.4481 - val_loss: 0.9577\n",
      "Epoch 7/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4484 - loss: 0.9591 - val_accuracy: 0.4488 - val_loss: 0.9556\n",
      "Epoch 8/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.4500 - loss: 0.9582 - val_accuracy: 0.4489 - val_loss: 0.9556\n",
      "Epoch 9/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.4496 - loss: 0.9572 - val_accuracy: 0.4474 - val_loss: 0.9558\n",
      "Epoch 10/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.4504 - loss: 0.9569 - val_accuracy: 0.4482 - val_loss: 0.9556\n",
      "Epoch 11/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.4494 - loss: 0.9568 - val_accuracy: 0.4475 - val_loss: 0.9550\n",
      "Epoch 12/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 23ms/step - accuracy: 0.4490 - loss: 0.9563 - val_accuracy: 0.4489 - val_loss: 0.9566\n",
      "Epoch 13/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4510 - loss: 0.9558 - val_accuracy: 0.4503 - val_loss: 0.9547\n",
      "Epoch 14/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - accuracy: 0.4500 - loss: 0.9563 - val_accuracy: 0.4492 - val_loss: 0.9553\n",
      "Epoch 15/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4503 - loss: 0.9551 - val_accuracy: 0.4465 - val_loss: 0.9546\n",
      "Epoch 16/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4509 - loss: 0.9560 - val_accuracy: 0.4488 - val_loss: 0.9550\n",
      "Epoch 17/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4508 - loss: 0.9549 - val_accuracy: 0.4482 - val_loss: 0.9537\n",
      "Epoch 18/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4512 - loss: 0.9548 - val_accuracy: 0.4484 - val_loss: 0.9542\n",
      "Epoch 19/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.4513 - loss: 0.9544 - val_accuracy: 0.4492 - val_loss: 0.9540\n",
      "Epoch 20/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.4500 - loss: 0.9547 - val_accuracy: 0.4481 - val_loss: 0.9544\n",
      "Epoch 21/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.4501 - loss: 0.9555 - val_accuracy: 0.4481 - val_loss: 0.9540\n",
      "Epoch 22/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.4518 - loss: 0.9547 - val_accuracy: 0.4502 - val_loss: 0.9539\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - accuracy: 0.4480 - loss: 0.9532\n",
      "Test Accuracy: 0.4482"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model, Encoders, and Scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "\n",
    "# -------------------------------------\n",
    "# 1. Load Dataset\n",
    "# -------------------------------------\n",
    "df = pd.read_csv('million_patients.csv')\n",
    "print(f\"Loaded dataset with {len(df):,} records\")\n",
    "print(\"Columns in dataset:\", df.columns.tolist())\n",
    "\n",
    "# -------------------------------------\n",
    "# 2. Drop Unnecessary Columns\n",
    "# -------------------------------------\n",
    "cols_to_drop = ['patient_id', 'admit_date', 'discharge_date', 'los']\n",
    "df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. Process Numeric Features with RobustScaler\n",
    "# -------------------------------------\n",
    "# Use the available numeric columns from the dataset.\n",
    "numeric_cols = ['age', 'bmi', 'glucose', 'systolic_bp', 'diastolic_bp', 'wbc']\n",
    "\n",
    "scaler = RobustScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "joblib.dump(scaler, 'robust_scaler.pkl')\n",
    "print(\"Numeric features scaled.\")\n",
    "\n",
    "# -------------------------------------\n",
    "# 4. Process Categorical Features using LabelEncoder\n",
    "# -------------------------------------\n",
    "categorical_cols = ['gender', 'region', 'diagnosis', 'category', 'medication', 'outcome']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "print(\"Categorical features encoded.\")\n",
    "\n",
    "# -------------------------------------\n",
    "# 5. Process 'symptoms' Using MultiLabelBinarizer\n",
    "# -------------------------------------\n",
    "# Assuming the 'symptoms' column contains comma-separated symptom strings\n",
    "df['symptoms'] = df['symptoms'].apply(lambda x: x.split(', '))\n",
    "mlb = MultiLabelBinarizer()\n",
    "symptom_encoded = pd.DataFrame(mlb.fit_transform(df['symptoms']), columns=mlb.classes_)\n",
    "joblib.dump(mlb, 'multi_label_binarizer.pkl')\n",
    "df = pd.concat([df.drop('symptoms', axis=1), symptom_encoded], axis=1)\n",
    "print(\"Symptoms encoded successfully.\")\n",
    "\n",
    "# -------------------------------------\n",
    "# 6. Build Feature Matrix and Target\n",
    "# -------------------------------------\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training size: {X_train.shape}, Testing size: {X_test.shape}\")\n",
    "\n",
    "# If your model requires dense arrays, convert the DataFrames to numpy arrays:\n",
    "X_train_dense = X_train.to_numpy()\n",
    "X_test_dense = X_test.to_numpy()\n",
    "\n",
    "# -------------------------------------\n",
    "# 7. Build the Deep Neural Network Model\n",
    "# -------------------------------------\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train_dense.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(np.unique(y)), activation='softmax')  # Output layer with number of unique diagnoses\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# -------------------------------------\n",
    "# 8. Train the Model\n",
    "# -------------------------------------\n",
    "history = model.fit(\n",
    "    X_train_dense, y_train,\n",
    "    validation_data=(X_test_dense, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=2048,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# -------------------------------------\n",
    "# 9. Evaluate the Model\n",
    "# -------------------------------------\n",
    "test_loss, test_accuracy = model.evaluate(X_test_dense, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# -------------------------------------\n",
    "# 10. Save the Model and Preprocessing Objects\n",
    "# -------------------------------------\n",
    "model.save('model2/disease_prediction_model.h5')\n",
    "joblib.dump(label_encoders, 'model2/label_encoders.pkl')\n",
    "joblib.dump(scaler, 'model2/robust_scaler.pkl')\n",
    "joblib.dump(mlb, 'model2/multi_label_binarizer.pkl')\n",
    "\n",
    "print(\"✅ Model, Encoders, and Scaler saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
